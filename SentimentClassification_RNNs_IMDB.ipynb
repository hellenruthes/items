{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "SentimentClassification_RNNs_IMDB.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellenruthes/items/blob/main/SentimentClassification_RNNs_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "j2tTkex1NJRj"
      },
      "source": [
        "## Sentiment Analysis of Reviews using RNNs in TensorFlow\n",
        "\n",
        "Modified from original code here: https://github.com/adeshpande3/LSTM-Sentiment-Analysis/blob/master/Oriole%20LSTM.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bMO3Ly_GNJRl"
      },
      "source": [
        "#### Some imports to make code compatible with Python 2 as well as 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "lABZPE3VNJRm"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "1Qh-02IxNJRm"
      },
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import tarfile\n",
        "import re"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "5gzFKSf8NJRn"
      },
      "source": [
        "from six.moves import urllib"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "IuvbAGdZNJRn"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "#"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1K0Up2oNJRn",
        "outputId": "35ac1e29-7e75-4c6e-b209-6bb622f5fdc0"
      },
      "source": [
        "print(np.__version__)\n",
        "print(mp.__version__)\n",
        "#print(tf.__version__)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.16.4\n",
            "2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gemZui8kROWw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP4aKkR3IpCT",
        "outputId": "7b14a2a3-33bb-45a7-9a7a-573266337ebd"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AadfhIEmNJRo"
      },
      "source": [
        "#### Download, unzip and untar files in an automated way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "WekG7nBxNJRp"
      },
      "source": [
        "DOWNLOADED_FILENAME = 'ImdbReviews.tar.gz'\n",
        "\n",
        "def download_file(url_path):\n",
        "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
        "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
        "\n",
        "    print('Found and verified file from this path: ', url_path)\n",
        "    print('Downloaded file: ', DOWNLOADED_FILENAME)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIsp25jZNJRp"
      },
      "source": [
        "### Extract reviews and the corresponding positive and negative labels from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZP4pwO2QNJRp"
      },
      "source": [
        "TOKEN_REGEX = re.compile(\"[^A-Za-z0-9 ]+\")\n",
        "\n",
        "\n",
        "def get_reviews(dirname, positive=True):\n",
        "    label = 1 if positive else 0\n",
        "\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(dirname):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(dirname + filename, 'r+') as f:\n",
        "                review = f.read().decode('utf-8')\n",
        "                review = review.lower().replace(\"<br />\", \" \")\n",
        "                review = re.sub(TOKEN_REGEX, '', review)\n",
        "                \n",
        "                reviews.append(review)\n",
        "                labels.append(label)\n",
        "    \n",
        "    return reviews, labels           \n",
        "\n",
        "def extract_labels_data():\n",
        "    # If the file has not already been extracted\n",
        "    if not os.path.exists('aclImdb'):\n",
        "        with tarfile.open(DOWNLOADED_FILENAME) as tar:\n",
        "            tar.extractall()\n",
        "            tar.close()\n",
        "        \n",
        "    positive_reviews, positive_labels = get_reviews(\"aclImdb/train/pos/\", positive=True)\n",
        "    negative_reviews, negative_labels = get_reviews(\"aclImdb/train/neg/\", positive=False)\n",
        "\n",
        "    data = positive_reviews + negative_reviews\n",
        "    labels = positive_labels + negative_labels\n",
        "\n",
        "    return labels, data"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0Pv0mSRNJRq",
        "outputId": "3a72e5da-fedf-4ba8-95c3-d3be6a96bad1"
      },
      "source": [
        "URL_PATH = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "download_file(URL_PATH)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found and verified file from this path:  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Downloaded file:  ImdbReviews.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bUPkCLRGNJRq"
      },
      "source": [
        "labels, data = extract_labels_data()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3-HSzHdNJRq",
        "outputId": "6f6c5cdd-1091-4969-9cd3-61b50a1ecbd1"
      },
      "source": [
        "labels[:5]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF0lKnA_NJRr",
        "outputId": "1b78a0b2-6dfb-44e2-d581-98778875cc03"
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'home room was a great movie if youve ever had drama in your life it keeps you wanting to see more wondering what the secret alicia is hiding i think i watched that movie 6 times in a row and never lost interest plus i usually dont cry over movies but this one made me cry each time i wish i could find more movies like that one all in all i thought it was a great movie the more you watch of it the more you become part of it the very end is the part that really got me when she cried when getting her diploma because it had her daughters name on it my heart felt as if it had shattered just then and how her new friend came to comfort her when she hadnt gotten hers yet i loved it so much',\n",
              " u'i lived in tokyo for 7 months knowing the reality of long train commutes bike rides from the train station soup stands and other typical scenes depicted so well certainly added to my own appreciation for this film which i really really liked there are aspects of japanese life in this film painted with vivid colors but you dont have to speak japanese to enjoy this movie director suos tricks were subtle for the most part i found his highlighting the character called tamako tamura with a soft filter making her sublime a tiny bit contrived but most of the directors tricks were so gentle that i was fully pulled in and just danced with his characters or cried or laughed aloud wonderful a',\n",
              " u'de grot is a terrific dutch thriller based on the book written by tim krabb another of his books het gouden ei was made into the great dutch mystery thriller called spoorloos the vanishing in 1988 this one is not as good as that thriller although much better than the american remake also called the vanishing but there are times it comes close  especially the opening moments are terrific we see a man later we learn his name is egon wagter fedja van hut coming from a plane in thailand when he picks up his bags it is pretty clear that he is smuggling something across the border these scenes are perfectly directed photographed and acted a kind of suspense is created that you would normally not have in an opening scene like this later we see how egon makes his deal in thailand with a woman both stating that they have never done anything like this  from this point the movie is constantly flashback and flashforward we see how egon still as a child here played by erik van der horst befriends a guy named axel as a kid played by benja bruijning we learn how they grew up as friends sort of and how axel as an adult played by marcel hensema became a criminal egon in the meanwhile goes to college and settles with a woman around this time he sometimes meets axel but does not really want anything to do with him  the movie is chronological in a way it shows egon and axel as kids than as students young adults and in their midthirties but from time to time like i said the movie goes back to when they were kids and jumps forward again every time we see them as kids it explains something that happens when they are adults  minor spoilers herein  the title means the cave and it is the cave that gives the movie its happy ending although it is in fact not that happy like the beginning the ending is terrific the middle part of the movie is entertaining and in a way it distracts our attention of the first scenes only to come back at that point in the end it is the editing that gives the movie its happy ending although we can say the dramatic ending is happy in a way as well',\n",
              " u'it was hard for me to believe all of the negative comments regarding this allstar flick i laughed through the entire picture as did my entire family the movie clearly defined itself as an old time gangster comedythe players were hystericalill bet they had a good old time while making it of course goldblum and dreyfuss were greatand how about those everly sisters each of the two falcos and the divine music throughout rob reiner made a great laughing limo driver and gabriel byrne a laughable neurotic not to mention gregory hines burt reynolds the sleepy joe character and the whole mortuary and grave digger references paul anka was his usual entertaining self with the added attraction of running scared after byrne decided to make a duet of his my way welcome home to vick performance  i am of the opinion that this movie was a comical tribute to frank sinatra and friends dreyfuss imitated him well i am also of the opinion that no one of any age would even think of imitating the actions which occurred in this movieits a jokenot a terrifying gangsta film the cars and clothing were impressive as was the decorative vics place  truly i think of mad dog time as a musical comedy less harmful than many cartoons tv crime dramas and talk shows i would recommend the video for an evening of family entertainment',\n",
              " u'without mental anachronism this film which i would like to find in dvd offer an extraordinary diving in the vital and mental context of thought of the people before the disenchantment of the world that there is thirty years a director and a scenario writer could test one such empathy and such a romantic truth to do it of them masterpiece leaves me astounding it would be necessary to be able to see and reexamine it film for better seizing than the temporal and cultural distance us to make lose of capacity to be includedunderstood analyze and finally to accept of such or such example of primitive thought because this thought maintaining almost impossible to feel in the secularized world however contain certain keys of our behavior that only them future generations will be able to analyze with sufficient relevance if somebody knows where i then to get a numerical copy or vhs to me or dvd thank you in advance']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54SfMOftNJRr",
        "outputId": "c2361e33-c24e-4db5-efa4-c4d435ec4620"
      },
      "source": [
        "len(labels), len(data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQI-lHDrNJRr",
        "outputId": "9788f721-912a-492d-ec73-cf853303c689"
      },
      "source": [
        "max_document_length = max([len(x.split(\" \")) for x in data])\n",
        "print(max_document_length)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWym3YiSNJRs"
      },
      "source": [
        "### How many words to consider in each review?\n",
        "\n",
        "Majority of the reviews fall under 250 words. This a number we've chosen based on some analysis of the data:\n",
        "\n",
        "* Count the number of words in each file and divide by number of files to get an average i.e. **avg_words_per_file = total_words / num_files**\n",
        "* Plot the words per file on matplot lib and try find a number which includes a majority of files\n",
        "\n",
        "Word embeddings all have the same dimensionality which you can specify. A document is a vector of word embeddings (one dbpedia instance is a document in this case)\n",
        "\n",
        "* Each document should be of the **same length**, documents longer than the MAX_SEQUENCE_LENGTH are truncated to this length\n",
        "* The other documents will be **padded** by a special symbol to be the same max length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5WgW384sNJRs"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 250"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mrZkl2lSdlN",
        "outputId": "5934a365-885b-49af-e073-b00b56484db5"
      },
      "source": [
        "#HR\n",
        "#load words list from google drive\n",
        "wordsList = np.load('/content/drive/My Drive/wordsList.npy')\n",
        "print('Loaded the word list!')\n",
        "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
        "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
        "wordVectors = np.load('/content/drive/My Drive/wordVectors.npy')\n",
        "print ('Loaded the word vectors!')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the word list!\n",
            "Loaded the word vectors!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJeQrlb5SgYg"
      },
      "source": [
        "batchSize = 24\n",
        "lstmUnits = 64\n",
        "numClasses = 2\n",
        "iterations = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "numDimensions = 300 #Dimensions for each word vector"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9V3prIJTO37",
        "outputId": "226c9be0-9729-413f-be1b-c33f2c3b26cd"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
        "input_data = tf.placeholder(tf.int32, [batchSize, MAX_SEQUENCE_LENGTH])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKq6JTrbToil"
      },
      "source": [
        "data = tf.Variable(tf.zeros([batchSize, MAX_SEQUENCE_LENGTH, numDimensions]),dtype=tf.float32)\n",
        "data = tf.nn.embedding_lookup(wordVectors,input_data)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8WCNN6ST9MS",
        "outputId": "aadc0233-902c-4ef4-f93d-8e27450b2661"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "lstmCell = tf.compat.v1.nn.rnn_cell.LSTMCell(lstmUnits)\n",
        "#lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
        "lstmCell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
        "#lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
        "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0301 00:14:02.557367 139849947465600 deprecation.py:323] From <ipython-input-60-2c7026752dd9>:7: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0301 00:14:02.706458 139849947465600 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0301 00:14:02.718758 139849947465600 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv5OTeKWIxK-"
      },
      "source": [
        "\n",
        "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
        "value = tf.transpose(value, [1, 0, 2])\n",
        "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
        "prediction = (tf.matmul(last, weight) + bias)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC2rRoOoJUGM"
      },
      "source": [
        "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bK65wbIJd5A",
        "outputId": "7dccffa9-a140-480f-a610-c92a15d89f44"
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0301 00:17:50.064332 139849947465600 deprecation.py:323] From <ipython-input-63-773132a4e1b5>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "f0WHXxBwJvOV",
        "outputId": "4fd02401-bb41-4f7e-84f5-ab41ff7b7d84"
      },
      "source": [
        "\n",
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, tf.train.latest_checkpoint('models'))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/client/session.py:1752: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-85ae95e5fcdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj4tLCFeNJRt"
      },
      "source": [
        "### Vocabulary processor\n",
        " \n",
        "http://tflearn.org/data_utils/\n",
        " \n",
        "Library to map every word which occurs in our dataset to a unique identifer. If there are 10023 words each will be assigned a unique id from 1-10023"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MHrj2szXNJRt"
      },
      "source": [
        "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrA-9NinNJRt"
      },
      "source": [
        "#### Transform every word to a representation using unique ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jJSutJYHNJRt",
        "outputId": "cd2b7041-12df-460f-dc63-dcb7369c60d1"
      },
      "source": [
        "x_data = np.array(list(vocab_processor.fit_transform(data)))\n",
        "y_output = np.array(labels)\n",
        "\n",
        "vocabulary_size = len(vocab_processor.vocabulary_)\n",
        "print(vocabulary_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "111526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "TVXWleRJNJRu",
        "outputId": "9b7389f0-dc6c-4b2e-93bf-6c55234f3a3e"
      },
      "source": [
        "data[3:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'this is easily the most underrated film inn the brooks cannon sure its flawed it does not give a realistic view of homelessness unlike say how citizen kane gave a realistic view of lounge singers or titanic gave a realistic view of italians you idiots many of the jokes fall flat but still this film is very lovable in a way many comedies are not and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive its not the fisher king but its not crap either my only complaint is that brooks should have cast someone else in the lead i love mel as a director and writer not so much as a lead',\n",
              " u'this is not the typical mel brooks film it was much less slapstick than most of his movies and actually had a plot that was followable leslie ann warren made the movie she is such a fantastic underrated actress there were some moments that could have been fleshed out a bit more and some scenes that could probably have been cut to make the room to do so but all in all this is worth the price to rent and see it the acting was good overall brooks himself did a good job without his characteristic speaking to directly to the audience again warren was the best actor in the movie but fume and sailor both played their parts well']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "U1xTwNMfNJRu",
        "outputId": "fd0e7c27-f0a1-4ecd-aac5-56714b6a347a"
      },
      "source": [
        "x_data[3:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[290,   3, 364,  10, 121, 365, 291, 366,  10, 168, 367, 368, 162,\n",
              "        369,   7, 370, 243, 286,   4, 371, 372,  53,  92, 373, 374, 375,\n",
              "        376, 377, 378,   4, 371, 372,  53, 379, 380,  93, 381, 378,   4,\n",
              "        371, 372,  53, 382, 146, 383,  83,  53,  10, 384, 385, 386, 103,\n",
              "        387, 290, 291,   3, 388, 389,  25,   4, 390,  83, 391, 238, 243,\n",
              "         61,  30, 392,  32, 206,  25,   4, 393,  17,  14,  53,  10, 121,\n",
              "        394, 395, 396,  53, 397,   3, 398, 399, 162, 243,  10, 400, 401,\n",
              "        103, 162, 243, 402, 403,  22, 404, 405,   3,  32, 168, 285, 301,\n",
              "        406, 407, 408,  25,  10,  28,  59, 252, 167,  13,   4, 409,  61,\n",
              "        410, 243, 411,  35,  13,   4,  28,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0],\n",
              "       [290,   3, 243,  10, 412, 167, 168, 291,   7, 413,  35, 414, 267,\n",
              "         38, 121,  53, 199, 415,  61, 416, 153,   4, 257,  32, 413, 417,\n",
              "        418, 223, 224, 419,  10, 358, 420,   3,  20,   4, 335, 365, 421,\n",
              "        422, 110,  14, 423,  32, 424, 301,  99, 425, 320,   4, 426, 193,\n",
              "         61,  14, 304,  32, 424, 427, 301,  99, 428,  30, 178,  10, 429,\n",
              "         30, 188, 411, 103,  56,  25,  56, 290,   3, 430,  10, 431,  30,\n",
              "        432,  61,  46,   7,  10, 433, 413, 311, 434, 168, 435, 114,   4,\n",
              "        311, 436, 151, 199, 437, 438,  30, 439,  30,  10, 440, 441, 224,\n",
              "        413,  10, 297, 442,  25,  10, 358, 103, 443,  61, 232, 444, 445,\n",
              "         49, 446, 447,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "kE-Xi7rrNJRu",
        "outputId": "16adfc14-fe3e-407d-83c9-ecbecea8853f"
      },
      "source": [
        "x_data[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
              "         14,  15,  16,  17,  18,  19,  20,  13,  21,  22,  23,  24,  25,\n",
              "         10,  26,  27,  28,  29,  30,  31,  32,   1,  33,  34,   3,  35,\n",
              "         36,  30,  37,  38,   3,  21,  10,  39,  30,  40,  41,  10,  42,\n",
              "         43,  44,  45,  46,  47,  48,  49,  50,  21,  51,  10,  52,  53,\n",
              "         10,  54,  55,  56,  57,  29,  53,  10,  58,  59,  60,  61,  49,\n",
              "         43,  62,  59,  63,  10,  64,  25,  65,   4,  66,  67,  68,  30,\n",
              "         69,  70,  10,  18,  59,  71,  72,   9,   2,   4,  73,  74,  75,\n",
              "         76,  77,  30,  78,  79,  53,  80,  21,  66,  81,  30,   1,   2,\n",
              "         59,  82,  32,  83,  84,  53,  22,  85,  86,  32,   1,   2,   3,\n",
              "         87,  88,  89,   4,  90,  32,   7,  91,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0],\n",
              "       [ 92,  93,  94,  13,  95,  96,  97,  98,  99, 100, 101, 102,  24,\n",
              "        103, 104,   4, 105,  30, 106, 107, 108,  10, 109,  32, 110, 111,\n",
              "        112, 113,  44, 114, 115, 116, 117,  30,  18, 118,  93, 119, 102,\n",
              "         10, 120, 121, 122,  86,  53,  10, 123,  13, 124,   4, 125, 126,\n",
              "        127, 128,  17, 129,  20,  13, 130,  10, 131, 108, 132, 133, 134,\n",
              "         30, 135, 136,  10, 137, 138,  93, 128, 139, 140, 141, 142,  30,\n",
              "        143, 144, 108,  10, 145, 103,  89, 139, 146, 110, 147,   4, 148,\n",
              "         30, 149, 108,  10, 145, 102,   4, 150, 151,  10, 152, 146, 111,\n",
              "        153, 116,   4, 154,  10, 155, 156,   4, 157, 158, 108,  10, 159,\n",
              "          4, 160,  61, 115, 146, 111, 161,  30,  46,  89, 162, 163,  30,\n",
              "        141, 123,  32,   3, 164, 165, 166, 167, 168,  44, 169,  44, 170,\n",
              "         13, 171, 172,   4, 173, 174,  44,  98, 115,  25,  10, 175, 176,\n",
              "        177,  30, 178,   4, 148, 179,   4, 180, 181, 182, 183,  30,  46,\n",
              "        139, 184,  45, 149,  25,  10, 145, 102, 185, 186, 151,  10, 152,\n",
              "        139, 171, 187, 184,  45, 188,  89, 184, 189, 179,   4, 190, 191,\n",
              "         53, 192, 193, 194,  10, 195, 108, 196, 171,   3, 197, 108,  10,\n",
              "        109, 179,   4, 198, 108, 199, 200,  30, 201, 199, 202, 203, 196,\n",
              "        184, 204, 205, 206,  10, 207, 208, 147,  10, 209, 210, 211,   4,\n",
              "        212, 213, 162, 214, 108, 199, 215, 196, 171, 216,  15, 217, 218,\n",
              "          4, 219, 211]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LFW9TLjeNJRv",
        "outputId": "034652c4-ddb6-4113-e8bc-b530701f9dfe"
      },
      "source": [
        "y_output[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Klsyy1NJRv"
      },
      "source": [
        "#### Shuffle the data so the training instances are randomly fed to the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "YjU4VgnxNJRv"
      },
      "source": [
        "np.random.seed(22)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(x_data)))\n",
        "\n",
        "x_shuffled = x_data[shuffle_indices]\n",
        "y_shuffled = y_output[shuffle_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "x0kkivS6NJRv"
      },
      "source": [
        "TRAIN_DATA = 5000\n",
        "TOTAL_DATA = 6000\n",
        "\n",
        "train_data = x_shuffled[:TRAIN_DATA]\n",
        "train_target = y_shuffled[:TRAIN_DATA]\n",
        "\n",
        "test_data = x_shuffled[TRAIN_DATA:TOTAL_DATA]\n",
        "test_target = y_shuffled[TRAIN_DATA:TOTAL_DATA]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "T34rzoWjNJRw"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.int32, [None, MAX_SEQUENCE_LENGTH])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "BBV-EQIuNJRw"
      },
      "source": [
        "num_epochs = 20\n",
        "batch_size = 25\n",
        "embedding_size = 50\n",
        "max_label = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lF_gChENJRw"
      },
      "source": [
        "### Embeddings to represent words\n",
        "\n",
        "These embeddings are generated as a part of the training process of the RNN. The embeddings are trained using the reviews in the training dataset.\n",
        "\n",
        "* *embedding_matrix* This is a matrix which holds the embeddings for every word in the vocabulary. The values are determined during the training process\n",
        "* *embeddings* The embeddings for the words which are input as a part of one training batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "sfGHZFqjNJRx"
      },
      "source": [
        "embedding_matrix = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "embeddings = tf.nn.embedding_lookup(embedding_matrix, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZhQAF98NJRx",
        "outputId": "672d4fbb-b843-4ef9-df74-9df4716a411a"
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(111526, 50) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DJG64ShfNJRx",
        "outputId": "23d4ef83-0502-4134-dcf0-8ea961220f5a"
      },
      "source": [
        "embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'embedding_lookup:0' shape=(?, 250, 50) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "NB5sP9BvNJRy"
      },
      "source": [
        "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size)\n",
        "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYd0CN8iNJRy"
      },
      "source": [
        "### Results from an RNN of LSTM cells\n",
        "\n",
        "(ouput, (**final_state**, other_state_info))\n",
        "\n",
        "We're interested in the final state of this RNN because those are the encodings we feed into the prediction layer of our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jzPixbLLNJRy"
      },
      "source": [
        "_, (encoding, _) = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9GpR7yDyNJRy",
        "outputId": "6ab380fa-0a90-4958-9334-e10c683380ae"
      },
      "source": [
        "encoding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'rnn_1/while/Exit_2:0' shape=(?, 50) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOdmi1s8NJRz"
      },
      "source": [
        "#### A densely connected prediction layer\n",
        "\n",
        "* *activation=None* because the activation will be part of the tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "* *cross_entropy* the loss function for probability distributions\n",
        "* *max_label* the number of outputs of the prediction layer, here is 2, positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "l5jh8VTwNJRz"
      },
      "source": [
        "logits = tf.layers.dense(encoding, max_label, activation=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "MEoz7SuENJRz"
      },
      "source": [
        "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
        "loss = tf.reduce_mean(cross_entropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGnNk4qmNJR0"
      },
      "source": [
        "#### Find the output with the highest probability and compare against the true label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "w_TMGNVTNJR0"
      },
      "source": [
        "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
        "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "H-FRCxkKNJR0"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(0.01)\n",
        "train_step = optimizer.minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "-pYs7pVbNJR0"
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VjTDkMBsNJR0",
        "outputId": "95e2f83d-e42c-4d5d-a48a-11b7814ecc34"
      },
      "source": [
        "with tf.Session() as session:\n",
        "    init.run()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        num_batches = int(len(train_data) // batch_size) + 1\n",
        "        \n",
        "        for i in range(num_batches):\n",
        "            # Select train data\n",
        "            min_ix = i * batch_size\n",
        "            max_ix = np.min([len(train_data), ((i+1) * batch_size)])\n",
        "\n",
        "            x_train_batch = train_data[min_ix:max_ix]\n",
        "            y_train_batch = train_target[min_ix:max_ix]\n",
        "            \n",
        "            train_dict = {x: x_train_batch, y: y_train_batch}\n",
        "            session.run(train_step, feed_dict=train_dict)\n",
        "            \n",
        "            train_loss, train_acc = session.run([loss, accuracy], feed_dict=train_dict)\n",
        "\n",
        "        test_dict = {x: test_data, y: test_target}\n",
        "        test_loss, test_acc = session.run([loss, accuracy], feed_dict=test_dict)    \n",
        "        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.5}'.format(epoch + 1, test_loss, test_acc)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Test Loss: 0.69, Test Acc: 0.49\n",
            "Epoch: 2, Test Loss: 0.8, Test Acc: 0.505\n",
            "Epoch: 3, Test Loss: 0.83, Test Acc: 0.602\n",
            "Epoch: 4, Test Loss: 0.8, Test Acc: 0.731\n",
            "Epoch: 5, Test Loss: 1.1, Test Acc: 0.759\n",
            "Epoch: 6, Test Loss: 1.3, Test Acc: 0.774\n",
            "Epoch: 7, Test Loss: 1.3, Test Acc: 0.796\n",
            "Epoch: 8, Test Loss: 1.3, Test Acc: 0.797\n",
            "Epoch: 9, Test Loss: 1.4, Test Acc: 0.799\n",
            "Epoch: 10, Test Loss: 1.5, Test Acc: 0.809\n",
            "Epoch: 11, Test Loss: 1.5, Test Acc: 0.813\n",
            "Epoch: 12, Test Loss: 1.5, Test Acc: 0.813\n",
            "Epoch: 13, Test Loss: 1.6, Test Acc: 0.813\n",
            "Epoch: 14, Test Loss: 1.6, Test Acc: 0.814\n",
            "Epoch: 15, Test Loss: 1.6, Test Acc: 0.819\n",
            "Epoch: 16, Test Loss: 1.7, Test Acc: 0.82\n",
            "Epoch: 17, Test Loss: 1.7, Test Acc: 0.82\n",
            "Epoch: 18, Test Loss: 1.8, Test Acc: 0.82\n",
            "Epoch: 19, Test Loss: 1.8, Test Acc: 0.818\n",
            "Epoch: 20, Test Loss: 1.9, Test Acc: 0.819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "thXsOZoZNJR1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}